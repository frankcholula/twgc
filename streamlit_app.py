import streamlit as st
import pandas as pd
import numpy as np
import re
import time

# Dashboard setup
pd.set_option("display.max_colwidth", None)
st.set_page_config(
    page_title="Taiwan Waste Management Data",
    page_icon="ğŸ—‘ï¸",
    layout="centered",
    initial_sidebar_state="auto",
)
placeholder = st.empty()
data_load_state = st.text("Loading data...")
STAT_P_126_DATA = "data/stat_p_126.csv"
STAT_P_126_METADATA = "data/STAT_P_126_Metadata.csv"


@st.cache_data(ttl=60)
def load_data(filepath: str, nrows=1000) -> pd.DataFrame:
    data = pd.read_csv(filepath, nrows=nrows)
    return data


def extract_metadata(df: pd.DataFrame) -> str:
    pattern = r"\(([\u4e00-\u9fff]+)\)"
    headers = metadata["ä¸»è¦è³‡æ–™æ¬„ä½"]
    headers = headers.str.findall(pattern).tolist()[0]
    return headers


data = load_data(STAT_P_126_DATA)
metadata = load_data(STAT_P_126_METADATA)
metadata_headers = extract_metadata(metadata)
data_load_state.text("Loading data...done!")


def get_cleaned_compost_data(data: pd.DataFrame) -> pd.DataFrame:
    mask = data["item1"].apply(lambda chinese_date: "æœˆ" in chinese_date)
    data = data[mask]

    def _convert_date_regex(chinese_date):
        # Extract the year and month from the Chinese date using regex
        match = re.match(r"(\d+)å¹´\s*(\d*)æœˆ*", chinese_date)

        if match:
            # The year is offset by 1911 because the original data is in the Republic of China calendar
            year = int(match.group(1)) + 1911
            month = match.group(2)
            # Convert the year and month to a datetime object
            return pd.to_datetime(f"{year}-{int(month)}")
        else:
            print(f"Problematic input: {chinese_date}")
            return None

    def _rename_columns(df: pd.DataFrame) -> pd.DataFrame:
        column_mapping = {
            "item1": "çµ±è¨ˆæœŸ",
            "value1": "ç¸½ç”¢ç”Ÿé‡",
            "value2": "ä¸€èˆ¬åƒåœ¾é‡",
            "value3": "è³‡æºåƒåœ¾é‡",
            "value4": "å»šé¤˜é‡",
            "value5": "å¹³å‡æ¯äººæ¯æ—¥ä¸€èˆ¬å»¢æ£„ç‰©ç”¢ç”Ÿé‡",
        }
        df = df.rename(columns=column_mapping)
        return df

    data = _rename_columns(data)
    data["æ—¥æœŸ"] = data["çµ±è¨ˆæœŸ"].apply(_convert_date_regex)
    data = data.drop(columns=["çµ±è¨ˆæœŸ"])
    data["å»šé¤˜é‡"] = pd.to_numeric(data["å»šé¤˜é‡"], errors="coerce")
    data = data.dropna()
    return data


cleaned_data = get_cleaned_compost_data(data)

# compost data by months
cdbm = cleaned_data.copy()
cdbm.set_index("æ—¥æœŸ", inplace=True)
cdbm = cdbm["å»šé¤˜é‡"].resample("MS").asfreq()
cdbm = cdbm.reset_index()
cdbm["æœˆ"] = cdbm["æ—¥æœŸ"].dt.month
cdbm_means = cdbm.groupby("æœˆ")["å»šé¤˜é‡"].mean()
avg_dwpp = cleaned_data.copy()

# getting average waste per person
latest_date = avg_dwpp["æ—¥æœŸ"].max()
prev_latest_date = avg_dwpp["æ—¥æœŸ"].nlargest(2).iloc[-1]
latest_avg_dwpp = avg_dwpp.loc[avg_dwpp["æ—¥æœŸ"] == latest_date, "å¹³å‡æ¯äººæ¯æ—¥ä¸€èˆ¬å»¢æ£„ç‰©ç”¢ç”Ÿé‡"].values[
    0
]
prev_avg_dwpp = avg_dwpp.loc[
    avg_dwpp["æ—¥æœŸ"] == prev_latest_date, "å¹³å‡æ¯äººæ¯æ—¥ä¸€èˆ¬å»¢æ£„ç‰©ç”¢ç”Ÿé‡"
].values[0]

# simulating live data by segmenting the data into yearly chunk
segmented_data = cleaned_data.copy()
segmented_data.sort_values(by="æ—¥æœŸ", ascending=True, inplace=True)
num_years = 12
partitioned = np.array_split(segmented_data, num_years)
# print(partitioned[0].size)
segments = [partitioned[0]]
# simulating live data
for i in range(num_years - 1):
    old_segments = segments[i]
    new_segment = partitioned[i + 1]
    segments.append(pd.concat([old_segments, new_segment]))

for year in range(11):
    segmented_data["ç¸½å»šé¤˜é‡"] = segments[year]["å»šé¤˜é‡"]
    time.sleep(1)

    with placeholder.container():
        st.title("ğŸšš Taiwan Waste Management Data")
        st.markdown("# å…¨åœ‹ä¸€èˆ¬å»¢æ£„ç‰©ç”¢ç”Ÿé‡")
        data_description_zh = metadata["è³‡æ–™é›†æè¿°"].to_string(index=False, header=False)
        data_description_en = "This dashboard consolidates comprehensive waste and recycling data from the Environmental Protection Administration of the Executive Yuan and local environmental protection agencies. It presents statistics on the generation of different waste types and provides insights into the average daily waste generated per person. The unit for the average daily waste per person is kilograms, while the remaining data is measured in metric tons."
        st.write(data_description_zh)
        st.write(data_description_en)
        kpi1, kpi2, kpi3 = st.columns(3)

        kpi1.metric(
            label="æ¯äººæ¯æ—¥ä¸€èˆ¬å»¢æ£„ç‰©ç”¢ç”Ÿé‡(kg) ğŸ—‘ï¸",
            value=float(latest_avg_dwpp),
            delta=round(float(latest_avg_dwpp) - float(prev_avg_dwpp), ndigits=3),
        )

        st.markdown("## å»šé¤˜é‡ Compost Data Over Time")
        st.markdown("## ")
        st.line_chart(data=segmented_data, x="æ—¥æœŸ", y="ç¸½å»šé¤˜é‡")

        st.markdown("## æ¯æœˆå¹³å‡å»šé¤˜é‡ Compost Data by Months")
        st.bar_chart(cdbm_means, x=cdbm_means.index.all(), y="å»šé¤˜é‡")
        fig3, fig4 = st.columns(2)
        with fig3:
            st.markdown("## Raw data")
            st.write(data)
        with fig4:
            st.markdown("## Cleaned data")
            st.write(cleaned_data)
